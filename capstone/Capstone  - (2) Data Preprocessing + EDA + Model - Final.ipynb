{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imported Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade, EasyEnsemble\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from nltk import ConfusionMatrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('./final_dataset/Combined_B.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical fraud is an ongoing issue everywhere in the world. Fraudulent behaviors such as unnecessary referrals and false claims for services could lead to the rising insurance premium, making healthcare more costly for the populations.\n",
    "\n",
    "The Ministry of Health (MOH) in Singapore takes a serious view of any attempts to defraud the healthcare system at the expense of the citizens. Together with insurers, MOH is investing resources in monitoring the landscape of MediShield Life, and Integrated Shield Plan (IP) claims and looking of ways to identify anomalous trends more effectively.\n",
    "\n",
    "The detection of medical fraud within healthcare in Singapore is traditionally found through manual effort by internal auditors searching through numerous records to find possibly suspicious behaviors. This manual process with massive amounts of data to sieve through data can be tedious and very inefficient. \n",
    "\n",
    "While the volume of information within healthcare continues to increase due to technological advancement, the government and insurers are also looking for more efficient ways to detect medical fraud.\n",
    "\n",
    "In this workbook, we will explore the use of supervised learning to detect medical fraud and techniques that can be possibly deployed to identify fraud more accurately and to minimize the number of false-positive signals.\n",
    "\n",
    "We processed our dataset with a number of different techniques to manage the imbalanced dataset:\n",
    "- train-test split with undersampled data (using near-miss technique)\n",
    "- train-test split with oversampled data (using SMOTE technique)\n",
    "- straified k-fold with undersampled data (using near-miss technique)\n",
    "- straified k-fold with oversampled data (using SMOTE technique)\n",
    "\n",
    "For each preprocessed dataset, they were evaluated using 3 learners:  Logistic Regression, AdaBoost and Random Forest. \n",
    "\n",
    "The best model was logistic regression model with SMOTE dataset (without stratified k fold). There is however still room to further improve the model by bringing down the false positive signals, and true positive cases. Next steps to explore:\n",
    "employ Spark for running and validating the models. The current dataset is not the full dataset, hence there are number of missing information that could have cause the misclassification. \n",
    "\n",
    "There is also needs to compare the cost of investigating false positive results to the cost of not identifying fraud providers - to ascertain on the threshold that is permissible. \n",
    "We also can improve the model by observing whether the action taken in response to the identified fraud case is correct.Outcomes data (this will capture any changes in fraud behaviours) can be used to update the model to continuously improving the model, and detect fraud more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As MediShield Life and Integrated Shield Plan claims data are not publicly available, we will be using real-world dataset from Medicare US as a replacement dataset.\n",
    "\n",
    "To minimize fraudulent activities in US, the Centers for Medicare and Medicaid Services (CMS) have been releasing yearly datasets for different parts of the Medicare program. In this workbook, we focus on the detection of Medicare fraud using Medicare Provider Utilization and Payment Data: Physicians and Other Supplier (Part B). \n",
    "\n",
    "We did a random sampling of 50,000 cases per year between 2015 and 2017 and mapped the real-word provider fraud labels using the List of Excluded Individuals and Entities (LEIE) from the Office of the Inspector General. Please refer to workbook - Data Preparation for more information.\n",
    "\n",
    "The final data-sample after mapping with LEIE is n=149,953."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data : (149953, 12)\n"
     ]
    }
   ],
   "source": [
    "## Lets Check Shape of datasets \n",
    "print('Shape of data :',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable      | Description    | \n",
    "| :------------- | :----------: | \n",
    "|npi| unique provier identification number|\n",
    "|  hcpcs_drug_indicator | Indicate if the HCPCS code for the specific service furnishedi s a HCPCS listed on the Medicare Part B Drug Average Sales Price (ASP) File.   | \n",
    "| line_srvc_cnt  | Number of services provided; note that the metrics used to count the number provided can vary from service to service. | \n",
    "| bene_unique_cnt | Number of distinct Medicare beneficiaries receiving the service. | \n",
    "| bene_day_srvc_cnt |Number of distinct Medicare beneficiary/per day services (unique count) | \n",
    "| average_Medicare_allowed_amt |Average of the Medicare allowed amount for the service; this figure is the sum of the amount Medicare pays, the deductible and coinsurance amounts that the beneficiary is responsible for paying, and any amounts that a third party is responsible for paying. | \n",
    "| average_submitted_chrg_amt |  Average of the charges that the provider submitted for the service. | \n",
    "| average_Medicare_payment_amt | Average amount that Medicare paid after deductible and coinsurance amounts have been deducted for the line item service.| \n",
    "| nppes_provider_gender | Gender of Provider| \n",
    "| provider_type| Medical provider's specialty (or practice)| \n",
    "|fraud| Fraud indicator (1) from LEIE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have created some basic functions for data preprocessing; mainly to save time for repeating the same type of preprocessing work for different datasets (if applicable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change variable labels\n",
    "def change_label (data):\n",
    "    data.columns = data.columns.str.strip().str.lower().str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if data has any missing values\n",
    "def check_cleaned(data):\n",
    "    var_not_cleaned = 0\n",
    "    var_cleaned =0\n",
    "    for i in range(len(data.columns)):\n",
    "        if data.iloc[:,i].count() <len(data.index):\n",
    "            print (\"NOT CLEANED: column: {} ,count: {}, dytype: {}\" .format(data.columns[i],data.iloc[:,i].count(),data[data.columns[i]].dtype))\n",
    "        if data.iloc[:,i].count() <len(data.index):\n",
    "            var_not_cleaned += 1\n",
    "        else: var_cleaned +=1\n",
    "    if var_cleaned ==len(data.columns):\n",
    "        print (\"DATA CLEANED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify object variables that need to create dummies\n",
    "def objects(data):\n",
    "    for i in range(len(data.columns)):\n",
    "        if data.iloc[:,i].dtype == \"object\":\n",
    "            print (\"column: {} ,count: {}, dytype: {}\" .format(data.columns[i],data.iloc[:,i].count(),data[data.columns[i]].dtype,data[data.columns[i]]))\n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for dummies variables created\n",
    "def check_dumb (data,dummiesvar):\n",
    "    for i in dummiesvar:\n",
    "        check_cols = ()\n",
    "        check_cols = [col for col in data.columns if i in col]\n",
    "        print (\"{},\".format(tuple(check_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to drop last dummies\n",
    "def to_dropdumb (var):\n",
    "    for i in range(len(var)):\n",
    "        to_drop=[]\n",
    "        to_drop=var[i][len(var[i])-1]\n",
    "        print (\"'{}',\" .format(to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if there are any more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT CLEANED: column: nppes_provider_gender ,count: 143368, dytype: object\n"
     ]
    }
   ],
   "source": [
    "check_cleaned(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nppes_provider_gender.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANED\n"
     ]
    }
   ],
   "source": [
    "check_cleaned(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We removed cases with hcpcs_drug_indicator = Y as these are not actual medical procedures. We also removed unnecessary variables such as allowed amount and standard amount, these figures are just an indication of medicare allowable amount. We also dropped variable such as gender and providers as these two variables do not help to boost the model scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149953, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df[df.hcpcs_drug_indicator !=\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"hcpcs_drug_indicator\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"average_medicare_allowed_amt\"], axis=1)\n",
    "df = df.drop([\"average_medicare_standard_amt\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"nppes_provider_gender\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_fraud =[\n",
    "'Clinical Laboratory',\n",
    "'Cardiology',\n",
    "'Internal Medicine',\n",
    "'Physical Therapist in Private Practice',\n",
    "'Anesthesiology',\n",
    "'Hematology-Oncology',\n",
    "'Family Practice',\n",
    "'Optometry',\n",
    "'Cardiovascular Disease (Cardiology)',\n",
    "'General Surgery',\n",
    "'Otolaryngology',\n",
    "'Obstetrics/Gynecology',\n",
    "'Physician Assistant',\n",
    "'Podiatry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.provider_type.isin(provider_fraud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"provider_type\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60395, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To find out no. of fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "01c007fa-0fcc-4eea-84ff-0861a2f8c533",
    "_kg_hide-input": true,
    "_uuid": "f6b96ff34855e3bf7af1f6979342b01c473e4e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.97 % of the dataset\n",
      "Frauds 0.03 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(df['fraud'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['fraud'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['npi', 'average_medicare_payment_amt', 'average_submitted_chrg_amt',\n",
       "       'bene_day_srvc_cnt', 'bene_unique_cnt', 'line_srvc_cnt', 'fraud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "558c9b60-3f52-4da5-92fa-9fc4acbdbb3a",
    "_uuid": "c2bb0945a312508e908386fc87adc227f0afe0e0"
   },
   "source": [
    "**Note:**  \n",
    "- 99.7% of the transactions are non-fraud.\n",
    "- If we use the original dataset for our predictive models and analysis, we most likely will get alot of erros and overfit since most transactions are not fraud. One of the key goals is to create a model to detect patterns that give signs of fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "657bc987-4b15-4cfa-b290-c39a2632e2ac",
    "_kg_hide-input": true,
    "_uuid": "337caaf6ed3f65beedb24a74eebb22d97ff52ba4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Fraud Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEoCAYAAACQD2yQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH2FJREFUeJzt3XuYXFWd7vHvawKCCiaY5mIuJmqrBA4X6QNRGI+CQoJKOBwYg0oC4kQBFdRHB7yhoEc9M0cUFcYwhCTIEXIAJSAYY8DBKJd0ALkkYFrApCeRBBOQuwZ+88deRXYq1d3V3au60vT7eZ56qvbaa69auwj19r7UWooIzMzMcnhZsztgZmYvHQ4VMzPLxqFiZmbZOFTMzCwbh4qZmWXjUDEzs2wcKmaJpGmSNmVqa7ikkHR0jvaq2v6opMdKy1+XdFfu90ltN2w/7KXJoWJNI2lO+sKqfuzX7L51RVJnqZ/PSlol6WpJ7y3Xi4hNwB7ADXW2u0TSd+vsxmXAm3rX87r68GNJPyuX9XY/zBwq1my/ovjSKj/urVVR0vYD2K/ufIWin28CjgdWA9dUh0JE/Dkinsv1pipsFxHPRMS6XO32JPd+2EubQ8Wa7bn0pVV+bIIX/3r/gaTvSFoP/Ecq/5ykeyQ9lY4cfiTp1ZUGq08PpbJ3p6OLEaWyk9KRxtOSFgC71tnnJ1I/V0XEbyPidOBTwOmS/iG1vcVpoxQIX5X0J0nPSVor6ZK07sfAwWn7ylHQmFKfJ0tqB54DDqu1f6mdj0lanfbnakmvKa3b6iikfNpM0teBDwFTS304pNbpL0n7SrpR0jOS/iJptqSdq99L0mckrZG0QdLFknYs1XmnpNvSf8PH0us96/z8bRvmULFt3QxgE3AI8JFU9jzFl/jewIcpvpDrPXUEgKS3AxcDFwL7UZzeObsf/ZwF/BX4X12s/0fgDODjQCtwFLA0rTsNuB24iM1Ha2tK234bOAt4C9DeRftvBD4AvB84HNgztVevbwFXAb8o9eG26kqSXgUsBDYCB1Ls7ztqvNe7KI7kDgU+CBwHfCK1sR1wDfBrYB9gEvB94IVe9Ne2UcOb3QEb8iZLerK0/JuImFJa7oiIz5c3iIjvlBYfknQmMF/SR6L+wezOABZGxDfT8h8kHUQRUr0WEZskrQRe30WV11EExaJ0JLaKFCoR8bikvwNPR8SfKxtIqrz8SkQsqlFetgMwPSI6U51TgJskTYiIh+ro/5OSngWGV/Wh+jviBGD79F5PpTofBxZJOrP0XhuB0yLieeB+SVcBhwH/AowEdgYWRMQfU/37e+qjDQ4+UrFmu5niSKHy+GjV+q3+Mk+nhRZL+k9JTwDzgR2Bll68757ALVVl1cu9JaCrULsC2IkiBP9d0rG9uEbU1dFJ2apKoCS3pr7kPqW0J/D7SqAkvy2tq7gvBUrFGtLpxXQ96MfAryRdJ+nTksZk7qc1iUPFmu3piOgoPf6zan35ywtJrweuA+6mOPVyAPBPaXXlS/oFii/4su2qlmv+ud9X6S/6VuDBWusj4k8Up4NOBZ4EzgOWSnpFHc0/1XOVrd+SLfexns+kHrWCM6qeAf5eo86L3zcRcQLwNmAJ8D8pjhTf3Yf+2DbGoWKDzX8HhgGfjYhbI+IPwOiqOuuBnSS9slRWfZvycopz+WXVy73xMYojkau6qpDu2ro2Is5I71W5ngDwN4r96qtxkl5bWq60WzmttJ7iOklZ9WdSTx+WA/tVfbaHpOcVdfYVgIi4KyK+FRHvoDjamd6b7W3b5FCxwWYlxbXAT0maIOlDwCer6twKPAN8U9IbJR1H8aVfdj7F9ZzPS2pN1wWOqrMPO0naXdJYSQdL+h7wPeC7EbGk1gaSPpIee0uaAJxI8dd8R6ryMHCQpNdJGiWpt/9vPgvMTXdmHQxcAFwTEZUjpxuBNkkz0mdyFnBQVRsPA/tIelPqQ61rrpdShM/ctC/vpLjZYX5EPFxPR9P7f1PS2ySNk3QYxU0Xy3u3y7YtcqjYoBIRdwCfAT5H8SV0IlB9IX89xQXlKcA9FHeNfaWqzhJgJkUg3Q28D/hand04B1hLEQiXA+OAoyPi091s81h6vyWpT1PTNqvS+v9DcYpqBcVRxWtrNdKNDuBq4HqK3/6spHR9KiJ+DnyD4k6yZRRHdz+qauNHabtlqQ9bHblFxJPAEcAuFDcaXA38hs2nIOvxFMWdbFel95sNzAH+tRdt2DZKnvnRzMxy8ZGKmZll41AxM7NsHCpmZpaNQ8XMzLJxqFg2acDE2c3ux2An6X5JX+pmfWWQxzGlso9K+tXA9HDbkga+DEm7p+Vj0wCVWX/gavVxqFgWknaluNX361Xlp0p6SMXcI8uURvHtZduVeVe+VFX+zlQ+qh/9Hq/ac7r8rOetB5f0eV2bhrcJSX0a50zF6Ma1PrP35e5zH10FvBI4ttkdGYocKpbLR4HbSz+2Q9IHKH4U+L+B/YHfATdIGteH9p8FPi+pN+N79cZktpzT5cRalVToy/Am24JXUfwm51MUP2Dsj/vYeh6cRbUq9mKMsyzSoKJzKPbTBphDxXL5ILCgquwzwJyIuCgiVkTEJyl+NHhKH9q/ieIX31/urpKkd6RTH89KekTSeXV+qf2lak6Xx1J7Xc1p0ippQXqPJ9NRWHl05coskWdUlW0xw6Ok3VI7z0h6WNKM+j6O3ouI6yLiixFxFV0PfFmvTTXmwXkOQNLlkq6U9GVJa4A/pvKT0uf0hKQ/p3q7VxpMn3GoGF6/UvaWVLZ3qez9kv6QPrObqD0y9ALgEHmgygHnULF+k7QLMJHSaLrpi/wA4JdV1X8JvL1Ub46kh+t4mxeAM4GPS3pDF/0YTTEvyp0UR0YnU8zM+M1a9Xupek6TnYCfA+9O73UNxeyPrb1s91JgAsW8I8ekPo/N0N8+Sae2NmVo6giKL/v3UIxsAMUAll8A9gWOBsZQ7H9v+vcGitNb11KMXXYRxVww1VZSjGLwP/rQd+sHz6diOYyjGL12balsFMXghI9U1X2E4ou4Yi3pL9meRMT1kn5LMdzItBpVTk3tnRoRLwArVMy18iNJX46Ip7tp/mZJ5UmipkTEb0rLW8xpAjwK3FFaPkfSURQjJ9f6ktuKpIkUX7qTIuK2VHYim8cDa4b11De3yX/TlvPg/DEi9i0t/xWYGREvjlYcEbNK6x+U9AngTkmjIuLROvt3GvBARHw2LT+QPscvlitFREhaC4yvs13LxKFiOVSmiX22xrrq0yxbDJ0eEWf18r0+D9wqqdY4UXsCt6RAqVhCMST+GymuJ3Tlg8C9peXqIfi3mNMknaL5KvBeiusJwykmyrq9513Yor+bym1HxIOSqoN4wEREZXDMnjzAlgNwVl+jubscKACSDqQYg20fiom6KmdKxlGEdD16Mw/OM2z+t2kDxKFiOVS+EEay+WjlUYppf3evqrsrWx+91C0ilqqYRfDbwLlVq7ubJKunawidEdHdEUL1nCbnUZyy+hzFkcXTwGVsntMFep7DpLJuMA7A97fefF6SRlBMQ3wt8CGKI6LRFINflufBgS0/s/7Mg7NLeh8bQL6mYjn8keJ0x8RKQUT8jWK02/dU1X0PxV1g/fEF4B8o7tgqWw68TVsOG38IxV/RdZ1i64VDKG5CuDoi7qaY2bD6gvEWc5hI2pFioq5yf4cDbaU6E4DdMvd1W7AXMAL454j4TUTcz9b7WQmA8rwvfZoHR9JOFEdAd1Svs8ZyqFi/pdNNv2LzZE0V3wFOTD/M21PFvCOvBf6tUkHFvBqLe/l+HcAs4PSqVRek9i9I7/deiusbP+jhekpf/AE4RtL+kvahOEp5eVWdG4ET0h1pewGXUJoEKyKWU3xuF0maJGn/VOeZzH0FilN2kvaTtB/FX/zj0vLYUp3TJd3bdSt99hDF/DGfkvT6dP3pK1V1lgN/prg+1Zrupjuzqs4FwJ6S/kXSmyVNo5jaoNrBwOPAbVn3wnrkULFcZgEfkFT+0rwCOAP4EnAXRegcmabWrdgDqHk3Vw/Oobge8aI0FfEUirux7qKYp+MnFEc2uZ0ObKSYsfDnwM1sfQT2jVR+LcWpn5vY+rrOdGA18GuKO8jmpuVGmERxZ9ydFKecvpFen12q00Jxh1tWEbGG4st/GkV4nAV8tqrOc2n9XhSf0xep+m+X/qA4jmIK4t9T3JxR67/v8cC8dMRsA8jzqVg2km4BLoiIXt0mar2jYkbGvwNjI6IzlX0UmBYRQ36edxXTKt8L7FP5fGzg+EjFcvoY/jdlzTce+CcHSnP47i/LJl2w7u62XbOGi4j+3ghi/eBQMRt8XgC+RnHHXcUd9H88L7N+8zUVMzPLxue/zcwsmyF3+mvUqFExfvz4ZnfDzGzQWLZs2aMRUde0E0MuVMaPH097e3vPFc3MDABJf+q5VsGnv8zMLBuHipmZZeNQMTOzbBwqZmaWjUPFzMyycaiYmVk2DQ0VSSMkXSnpfkkrJL1N0i6SFklamZ5HprqSdL6kDkl3S3prqZ0Zqf5KSTNK5QdIuidtc76k3swKZ2ZmmTX6SOV7wC8i4i3AvsAKikl3FkdEK7CYzZPwTAFa02MmcCGApF0o5ns4CDgQOLsSRKnOzNJ21TMBmpnZAGpYqEjaGXgHcDEU08tGxGPAVIqJiEjPR6fXUykm1YmIuBUYIWkP4AhgUURsiIiNwCJgclq3c0TcEsUAZvNKbZmZWRM08hf1r6eYc/oSSftSzFd+OrBbRKwFiIi1knZN9Uez5Yx3namsu/LOGuUNNW7ckka/hQ1Cq1ZVz6RsNjQ18vTXcOCtwIURsT/wFFvPN11W63pI9KF864almZLaJbWvX7+++16bmVmfNTJUOoHOiLgtLV9JETKPpFNXpOd1pfpjS9uPAdb0UD6mRvlWImJWRLRFRFtLS11jopmZWR80LFQi4s/AaklvTkWHAcuBBUDlDq4ZwDXp9QJgeroLbBLweDpNthA4XNLIdIH+cGBhWveEpEnprq/ppbbMzKwJGj1K8SeByyRtDzwInEQRZPMlnQysAo5Lda8HjgQ6gKdTXSJig6RzgaWp3jkRsSG9PgWYA+wI3JAeZmbWJA0NlYi4C2irseqwGnUDOK2LdmYDs2uUtwN797ObZmaWiX9Rb2Zm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2TQ0VCQ9LOkeSXdJak9lu0haJGlleh6ZyiXpfEkdku6W9NZSOzNS/ZWSZpTKD0jtd6Rt1cj9MTOz7g3Ekcq7ImK/iGhLy2cCiyOiFViclgGmAK3pMRO4EIoQAs4GDgIOBM6uBFGqM7O03eTG746ZmXWlGae/pgJz0+u5wNGl8nlRuBUYIWkP4AhgUURsiIiNwCJgclq3c0TcEhEBzCu1ZWZmTdDoUAngl5KWSZqZynaLiLUA6XnXVD4aWF3atjOVdVfeWaPczMyaZHiD2z84ItZI2hVYJOn+burWuh4SfSjfuuEi0GYCjBs3rvsem5lZnzX0SCUi1qTndcBPKa6JPJJOXZGe16XqncDY0uZjgDU9lI+pUV6rH7Mioi0i2lpaWvq7W2Zm1oWGhYqkV0raqfIaOBy4F1gAVO7gmgFck14vAKanu8AmAY+n02MLgcMljUwX6A8HFqZ1T0ialO76ml5qy8zMmqCRp792A36a7vIdDvy/iPiFpKXAfEknA6uA41L964EjgQ7gaeAkgIjYIOlcYGmqd05EbEivTwHmADsCN6SHmZk1ScNCJSIeBPatUf4X4LAa5QGc1kVbs4HZNcrbgb373VkzM8vCv6g3M7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpZNw0NF0jBJd0q6Li1PkHSbpJWSrpC0fSp/eVruSOvHl9o4K5U/IOmIUvnkVNYh6cxG74uZmXVvII5UTgdWlJa/DZwXEa3ARuDkVH4ysDEi3gicl+ohaSIwDdgLmAxckIJqGPBDYAowETg+1TUzsyZpaKhIGgO8F/j3tCzgUODKVGUucHR6PTUtk9YflupPBS6PiOci4iGgAzgwPToi4sGI+BtweaprZmZN0ugjle8CnwdeSMuvAR6LiE1puRMYnV6PBlYDpPWPp/ovlldt01W5mZk1ScNCRdL7gHURsaxcXKNq9LCut+W1+jJTUruk9vXr13fTazMz649GHqkcDBwl6WGKU1OHUhy5jJA0PNUZA6xJrzuBsQBp/auBDeXyqm26Kt9KRMyKiLaIaGtpaen/npmZWU0NC5WIOCsixkTEeIoL7TdGxIeAm4BjU7UZwDXp9YK0TFp/Y0REKp+W7g6bALQCtwNLgdZ0N9n26T0WNGp/zMysZ8N7rpLdPwOXS/o6cCdwcSq/GLhUUgfFEco0gIi4T9J8YDmwCTgtIp4HkPQJYCEwDJgdEfcN6J6YmdkWVBwMDB1tbW3R3t7e5+3HjVuSsTf2UrFq1SHN7oJZw0haFhFt9dT1L+rNzCwbh4qZmWXjUDEzs2wcKmZmlo1DxczMsnGomJlZNg4VMzPLxqFiZmbZOFTMzCwbh4qZmWXjUDEzs2zqChVJi+spMzOzoa3bUYol7QC8AhglaSSbJ8baGXhtg/tmZmaDTE9D338MOIMiQJaxOVT+Cvywgf0yM7NBqNtQiYjvAd+T9MmI+P4A9cnMzAapuibpiojvS3o7ML68TUTMa1C/zMxsEKorVCRdCrwBuAt4PhUH4FAxM7MX1TudcBswMYbaNJFmZtYr9f5O5V5g90Z2xMzMBr96j1RGAcsl3Q48VymMiKMa0iszMxuU6g2VrzayE2Zm9tJQ791f/9HojpiZ2eBX791fT1Dc7QWwPbAd8FRE7NyojpmZ2eBT75HKTuVlSUcDBzakR2ZmNmj1aZTiiPgZcGjmvpiZ2SBX7yjFx5Qex0r6FptPh3W1zQ6Sbpf0e0n3SfpaKp8g6TZJKyVdIWn7VP7ytNyR1o8vtXVWKn9A0hGl8smprEPSmX3YfzMzy6jeI5X3lx5HAE8AU3vY5jng0IjYF9gPmCxpEvBt4LyIaAU2Aien+icDGyPijcB5qR6SJgLTgL2AycAFkoZJGkYxqOUUYCJwfKprZmZNUu81lZN623D69f2TaXG79AiK02YfTOVzKW5XvpAipL6ayq8EfiBJqfzyiHgOeEhSB5uv53RExIMAki5PdZf3tq9mZpZHvae/xkj6qaR1kh6RdJWkMXVsN0zSXcA6YBHwR+CxiNiUqnQCo9Pr0cBqgLT+ceA15fKqbboqNzOzJqn39NclwAKKeVVGA9emsm5FxPMRsR8whuLoYs9a1dKzuljX2/KtSJopqV1S+/r163vqtpmZ9VG9odISEZdExKb0mAO01PsmEfEY8GtgEjBCUuW02xhgTXrdCYwFSOtfDWwol1dt01V5rfefFRFtEdHW0lJ3t83MrJfqDZVHJX24coFc0oeBv3S3gaQWSSPS6x2BdwMrgJuAY1O1GcA16fWCtExaf2O6LrMAmJbuDpsAtAK3A0uB1nQ32fYUF/MX1Lk/ZmbWAPWO/fUR4AcUd2UF8Dugp4v3ewBz011aLwPmR8R1kpYDl0v6OnAncHGqfzFwaboQv4EiJIiI+yTNp7gAvwk4LSKeB5D0CWAhMAyYHRH31bk/ZmbWAKpnihRJc4EzImJjWt4F+NeI+EiD+5ddW1tbtLe393n7ceOWZOyNvVSsWnVIs7tg1jCSlkVEWz116z39tU8lUAAiYgOwf186Z2ZmL131hsrLJI2sLKQjlXpPnZmZ2RBRbzD8X+B3kq6kuKbyj8A3GtYrMzMblOr9Rf08Se0Uv4YXcExE+JfrZma2hbpPYaUQcZCYmVmX+jT0vZmZWS0OFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsm4aFiqSxkm6StELSfZJOT+W7SFokaWV6HpnKJel8SR2S7pb01lJbM1L9lZJmlMoPkHRP2uZ8SWrU/piZWc8aeaSyCfhsROwJTAJOkzQROBNYHBGtwOK0DDAFaE2PmcCFUIQQcDZwEHAgcHYliFKdmaXtJjdwf8zMrAcNC5WIWBsRd6TXTwArgNHAVGBuqjYXODq9ngrMi8KtwAhJewBHAIsiYkNEbAQWAZPTup0j4paICGBeqS0zM2uCAbmmImk8sD9wG7BbRKyFIniAXVO10cDq0madqay78s4a5WZm1iQNDxVJrwKuAs6IiL92V7VGWfShvFYfZkpql9S+fv36nrpsZmZ91NBQkbQdRaBcFhFXp+JH0qkr0vO6VN4JjC1tPgZY00P5mBrlW4mIWRHRFhFtLS0t/dspMzPrUiPv/hJwMbAiIr5TWrUAqNzBNQO4plQ+Pd0FNgl4PJ0eWwgcLmlkukB/OLAwrXtC0qT0XtNLbZmZWRMMb2DbBwMnAPdIuiuVfQH4FjBf0snAKuC4tO564EigA3gaOAkgIjZIOhdYmuqdExEb0utTgDnAjsAN6WFmZk3SsFCJiCXUvu4BcFiN+gGc1kVbs4HZNcrbgb370U0zM8vIv6g3M7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpZNw0JF0mxJ6yTdWyrbRdIiSSvT88hULknnS+qQdLekt5a2mZHqr5Q0o1R+gKR70jbnS1Kj9sXMzOrTyCOVOcDkqrIzgcUR0QosTssAU4DW9JgJXAhFCAFnAwcBBwJnV4Io1ZlZ2q76vczMbIA1LFQi4mZgQ1XxVGBuej0XOLpUPi8KtwIjJO0BHAEsiogNEbERWARMTut2johbIiKAeaW2zMysSQb6mspuEbEWID3vmspHA6tL9TpTWXflnTXKzcysibaVC/W1rodEH8prNy7NlNQuqX39+vV97KKZmfVkoEPlkXTqivS8LpV3AmNL9cYAa3ooH1OjvKaImBURbRHR1tLS0u+dMDOz2gY6VBYAlTu4ZgDXlMqnp7vAJgGPp9NjC4HDJY1MF+gPBxamdU9ImpTu+ppeasvMzJpkeKMalvQT4J3AKEmdFHdxfQuYL+lkYBVwXKp+PXAk0AE8DZwEEBEbJJ0LLE31zomIysX/UyjuMNsRuCE9zMysiRoWKhFxfBerDqtRN4DTumhnNjC7Rnk7sHd/+mhmZnltKxfqzczsJcChYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy2bQh4qkyZIekNQh6cxm98fMbCgb1KEiaRjwQ2AKMBE4XtLE5vbKzGzoGtShAhwIdETEgxHxN+ByYGqT+2RmNmQN9lAZDawuLXemMjMza4Lhze5AP6lGWWxVSZoJzEyLT0p6oKG9GjpGAY82uxPbAtX6l2jN5n+f+byu3oqDPVQ6gbGl5THAmupKETELmDVQnRoqJLVHRFuz+2FWi/99NsdgP/21FGiVNEHS9sA0YEGT+2RmNmQN6iOViNgk6RPAQmAYMDsi7mtyt8zMhqxBHSoAEXE9cH2z+zFE+ZSibcv877MJFLHVdW0zM7M+GezXVMzMbBviULE+8fA4tq2SNFvSOkn3NrsvQ5FDxXrNw+PYNm4OMLnZnRiqHCrWFx4ex7ZZEXEzsKHZ/RiqHCrWFx4ex8xqcqhYX9Q1PI6ZDT0OFeuLuobHMbOhx6FifeHhccysJoeK9VpEbAIqw+OsAOZ7eBzbVkj6CXAL8GZJnZJObnafhhL/ot7MzLLxkYqZmWXjUDEzs2wcKmZmlo1DxczMsnGomJlZNg4VswaR9ClJKyRdlrndd0q6LmebZrkM+pkfzbZhpwJTIuKhSoGk4el3PmYvST5SMWsASf8GvB5YIOlxSbMk/RKYJ2m8pN9IuiM93p622eIIRNIPJJ2YXk+WdL+kJcAxTdgls7r4SMWsASLi45ImA++iGH3g/cAhEfGMpFcA74mIZyW1Aj8B2rpqS9IOwEXAoUAHcEXDd8Csj3ykYjYwFkTEM+n1dsBFku4B/j/FRGfdeQvwUESsjGIIjB83sJ9m/eIjFbOB8VTp9aeBR4B9Kf6wezaVb2LLP/R2KL32eEo2KPhIxWzgvRpYGxEvACcAw1L5n4CJkl4u6dXAYan8fmCCpDek5eMHtLdmveBQMRt4FwAzJN0KvIl0FBMRq4H5wN3AZcCdqfxZYCbw83Sh/k/N6LRZPTxKsZmZZeMjFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTb/BetwP1RY43hcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('fraud', data=df, palette=colors)\n",
    "plt.title('Fraud Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
    "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421"
   },
   "source": [
    "##### Lets investigate if there is any distinct differences between fraud and non-fraud cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "fig.tight_layout()\n",
    "\n",
    "#five categorical columns and three numerical columns of interest\n",
    "for i, category in enumerate([\"fraud\"]):   \n",
    "    ax = ax\n",
    "    df.groupby(\"fraud\").mean()[[\n",
    "\"average_medicare_payment_amt\",\n",
    "\"average_submitted_chrg_amt\",\"line_srvc_cnt\",\"bene_unique_cnt\"]].plot.bar(rot=0, ax=ax)\n",
    "\n",
    "#Rotating xticks for all\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    fig.tight_layout()\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(round(p.get_height(),0)), (p.get_x() * 1, p.get_height() * 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interestingly, we see that average charges submitted is considerably lower for fraud cases than non fraud cases.  Now, lets see the average total submitted charges for all services per provider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total_medicare_payment_amt\"] = df.line_srvc_cnt*df.average_medicare_payment_amt\n",
    "df[\"total_submitted_chrg_amt\"] = df.line_srvc_cnt*df.average_submitted_chrg_amt\n",
    "#df[\"total_medicare_payment_amt(sq)\"] = df.average_medicare_payment_amt*df.average_medicare_payment_amt\n",
    "#df[\"total_submitted_chrg_amt(sq)\"] = df.average_submitted_chrg_amt*df.average_submitted_chrg_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "fig.tight_layout()\n",
    "\n",
    "#five categorical columns and three numerical columns of interest\n",
    "for i, category in enumerate([\"fraud\"]):   \n",
    "    ax = ax\n",
    "    df.groupby(\"fraud\").mean()[[\n",
    "\"total_medicare_payment_amt\",\n",
    "\"total_submitted_chrg_amt\"]].plot.bar(rot=0, ax=ax)\n",
    "\n",
    "#Rotating xticks for all\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    fig.tight_layout()\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(round(p.get_height(),0)), (p.get_x() * 1, p.get_height() * 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The average total charges submitted by fraud providers is approx 40% higher than non fraud providers! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74903f3b-dc6b-40ba-abc8-86c3df5ca46e",
    "_uuid": "0b365b10bd363c23068accc448509ced879f1670"
   },
   "source": [
    "## How is the spread of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_ = df[[\n",
    "\"total_medicare_payment_amt\",\n",
    "\"total_submitted_chrg_amt\",\n",
    "\"bene_day_srvc_cnt\",\n",
    "\"bene_unique_cnt\",\n",
    "\"line_srvc_cnt\",\"fraud\"]]\n",
    "\n",
    "f, axes = plt.subplots(ncols=4, figsize=(25,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "#ax = sns.boxplot(x=\"fraud\", y=\"total_medicare_allowed_amt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[0])\n",
    "ax = sns.boxplot(x=\"fraud\", y=\"total_medicare_payment_amt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[0])\n",
    "#ax = sns.boxplot(x=\"fraud\", y=\"total_medicare_standard_amt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[2])\n",
    "ax = sns.boxplot(x=\"fraud\", y=\"total_submitted_chrg_amt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[1])\n",
    "ax = sns.boxplot(x=\"fraud\", y=\"bene_day_srvc_cnt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[2])\n",
    "ax = sns.boxplot(x=\"fraud\", y=\"line_srvc_cnt\", hue=\"fraud\", data=box_, palette=\"Set3\", ax=axes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['fraud','npi'], axis=1)\n",
    "y = df['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60395, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we are going to explore few approaches/techniques to handle the imbalanced dataset:\n",
    "1. Train-Test Split, followed by undersampling/oversampling of the train dataset\n",
    "2. Perform stratified k-folds and perform undersampling/oversamplng during k-folds (i.e. under/oversampling of each train dataset in each fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "956d34b9-e562-4b70-a2f8-fbe060273a83",
    "_uuid": "cc554c4ffec656cb38d01c034f2cd338e1cb4565"
   },
   "source": [
    "## Undersampling (non fraud cases) Train Dataset:\n",
    "reference: https://knowledgengg.wordpress.com/2019/03/04/this-is-suresh/\n",
    "\n",
    "There are a number of undersampling techniques to consider:\n",
    "- RANDOM UNDERSAMPLING : As explained by its name, this technique will randomly remove the samples from the majority class of sample. Disadvantage with this method is there is always a good amount of probability of loosing the actual information and retaining the outliers in the samples in most of the cases. \n",
    "<br><b>Verdict</b>: We will not use this technique for obvious reason. :)\n",
    "<br>\n",
    "\n",
    "- NEARMISS – 1: Mainly based on K-Nearest Neighbors approach. In Nearmiss algorithm the technique used is to calculate the mean distances from majority class to the minority class and retain the points whose mean distance from majority class to minority class is lowest by ranking them in order. It will remove data points in majority class which are farthest from the minority class. This Nearmiss-1 will try to retain the data that is close to the decision boundary. It works well on more scattered data. K or size of neighbor hood will be our hyper parameter . \n",
    "<br><b>Verdict</b>: We will skip this as well since our dataset is not very scattered.\n",
    "<br>\n",
    "\n",
    "- NEARMISS – 2: will work similar to Nearmiss-1. Nearmiss 2 algorithm works by , instead of looking of K-Nearest points, it will look at the K-farthest points and rank them in the farthest first order. Based on the sampling ratio provided it will remove the first n points from the majority class. This is more helpful in removing outliers and re sampled data is more concentrated in the center. \n",
    "<br><b>Verdict</b>: We will use this approach.\n",
    "<br>\n",
    "\n",
    "- NEARMISS – 3 technique will pick K-nearest Neighbors to each points in the minority class and retain those majority class points which lie in those neighborhood . Here we cannot choose the ratio, even if we choose the ratio that won’t be of much help as neighborhood defines the ratio. If we use this technique the resultant data will have more overlapping between the classes and won’t be of much help if we are using logistic regression model. \n",
    "<br><b>Verdict</b>: Doesn't sound nice to have too much overlapping between the classes. NEXT!\n",
    "<br>\n",
    "\n",
    "- CNN(condensed nearest neighbor) try to reduce or remove the points in the majority class which are closer to the points in the majority class. CNN algorithm randomly selects a point in random and scans for all the nearest neighbor from that point and try to eliminate them. This process will be repeated until the sample ratio is reached or there is minimum balance between the class is reached. As this process is done by selecting a point in random points as a result whole process will go on random. Because of this reason this algorithm is a high variance one. Over all those this will take longest of all the re sampling algorithms. \n",
    "<br><b>Verdict</b>: This would be challenging as our dataset is quite big. We will not use this as we cannot specify the ratio. Most likley the resultant data will be still extremely unbalanced. \n",
    "<br>\n",
    "\n",
    "- Edited nearest neighbour (ENN) ENN algorithm will remove the nearest neighbors whose class label is different from the majority K-nearest neighbors. The process is repeated until none of the points can be edited from the sample set. This is called Repeated ENN. This method is one of the most commonly used.\n",
    "<br><b>Verdict</b>: We will not use this as we cannot specify the ratio. Most likley the resultant data will be still extremely unbalanced. \n",
    "<br>\n",
    "\n",
    "- Tomek link is a pair of points in the sample, which are each other’s Nearest Neighbors but labeled as different class. This technique is basically used to remove the noisy and borderline example from the sample to get a better decision boundary. A variant of this will only remove the Tomek link of the majority class.\n",
    "<br><b>Verdict</b>: We will not use this as we cannot specify the ratio. Most likley the resultant data will be still extremely unbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "f0acfc44-eb2a-4356-ad03-d0c12807acd7",
    "_kg_hide-input": true,
    "_uuid": "e3a2b89752681164f14c8273452fc66734d7f41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of class labels before resampling Counter({0: 42262, 1: 14})\n",
      "Distribution of class labels after resampling Counter({0: 7000, 1: 14})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours, TomekLinks\n",
    "# Edited Nearest Neighbor (ENN)\n",
    "\n",
    "us = NearMiss(ratio=0.002, n_neighbors=3, version=2, random_state=42)\n",
    "X_train_NM2, y_train_NM2 = us.fit_sample(X_train, y_train)\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_NM2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling (fraud cases) Train Dataset:\n",
    "reference: https://knowledgengg.wordpress.com/2019/03/04/this-is-suresh/\n",
    "\n",
    "There are a number of oversampling techniques to consider:\n",
    "- RANDOM OVERSAMPLING : Random Over sampling will randomly generate the samples by adding little bit of jitters to the existing samples in the minority class. This may cause overlapping of the data points and not much helpful always. This may lead to overfitting. \n",
    "<br><b>Verdict</b>: We will not use this technique for obvious reason. :)\n",
    "<br>\n",
    "\n",
    "- SYMMETRIC MINORITY OVERSAMPLING TECHNIQUE :(SMOTE): SMOTE will randomly select a sample from the minority class then finds a nearest Neighbor to it and draws a imaginary line between the sample points , crates new sample point on that line. This process is repeated until the desired ratio of the classes is reached.\n",
    "<br><b>Verdict</b>: Good to consider.\n",
    "<br>\n",
    "\n",
    "- SMOTE + TOMEK LINK REMOVAL : This Technique will combine the SMOTE and TOMEK together. This technique will under sample the majority class as well as over sample the minority class using both of the techniques together.\n",
    "<br><b>Verdict</b>: This will be much useful with the data sets with huge data imbalance and also deals with the untidy data and noisy data. We will use this!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of class labels before resampling Counter({0: 42262, 1: 14})\n",
      "Distribution of class labels after resampling Counter({0: 42259, 1: 8449})\n"
     ]
    }
   ],
   "source": [
    "os_us = SMOTETomek(ratio=0.2,random_state=42)\n",
    "X_train_SMTO, y_train_SMTO = os_us.fit_sample(X_train, y_train)\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_SMTO)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a59c8c8d-a4bc-4671-aa2f-9f959c142cde",
    "_uuid": "5119c4ea9e0b9031dbc5937b56323da224985024"
   },
   "source": [
    "### Splitting the Data using StratifiedKFold & perform undersampling/oversampling for each training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "c6c962cc-6f38-4a00-bcd7-ce9d91db954c",
    "_kg_hide-input": true,
    "_uuid": "9f7b5d920703b3a3c8c0f62bc6042e4615bc8324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [12076 12077 12078 ... 60392 60393 60394] Test: [    0     1     2 ... 12245 14798 17791]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [12076 12077 12078 ... 24155 24156 24157]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [24158 24159 24160 ... 36419 36554 42218]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [36234 36235 36236 ... 60388 60389 60390]\n",
      "Train: [    0     1     2 ... 60388 60389 60390] Test: [48312 48313 48314 ... 60392 60393 60394]\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kfold.split(X,y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train_KF, X_test_KF = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_KF, y_test_KF = y.iloc[train_index], y.iloc[test_index]\n",
    "    os_us = SMOTETomek(ratio=0.2,random_state=42)\n",
    "    X_train_KF_SMTO, y_train_KF_SMTO = os_us.fit_sample(X_train_KF, y_train_KF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [12076 12077 12078 ... 60392 60393 60394] Test: [    0     1     2 ... 12245 14798 17791]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [12076 12077 12078 ... 24155 24156 24157]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [24158 24159 24160 ... 36419 36554 42218]\n",
      "Train: [    0     1     2 ... 60392 60393 60394] Test: [36234 36235 36236 ... 60388 60389 60390]\n",
      "Train: [    0     1     2 ... 60388 60389 60390] Test: [48312 48313 48314 ... 60392 60393 60394]\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kfold.split(X,y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train_KF2, X_test_KF2 = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_KF2, y_test_KF2 = y.iloc[train_index], y.iloc[test_index]\n",
    "    us = NearMiss(ratio=0.002, n_neighbors=3, version=2, random_state=42)\n",
    "    X_train_KF_NM2, y_train_KF_NM2 = us.fit_sample(X_train_KF2, y_train_KF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#results = pd.DataFrame(columns = ['Model', \"Best_Param\", 'Accuracy', 'Precision', 'Recall', \"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\", 'F1 Score', \"ROC AUC\"])\n",
    "results =[]\n",
    "\n",
    "class MODEL_EVA:\n",
    "   \n",
    "    def __init__(self, model, best_param, y_test_, y_pred, y_pred_prob):\n",
    "        self.model = model\n",
    "        self.best_param = best_param\n",
    "        self.y_test_ = y_test_\n",
    "        self.y_pred = y_pred\n",
    "        self.y_pred_prob = y_pred_prob\n",
    "        \n",
    "    def confusion_matrix (self):\n",
    "        self.accuracy  = accuracy_score(self.y_test_, self.y_pred)\n",
    "        self.precision = precision_score(self.y_test_, self.y_pred)\n",
    "        self.recall    = recall_score(self.y_test_, self.y_pred)\n",
    "        self.f1        = 2 * (self.precision * self.recall) / (self.precision + self.recall)\n",
    "        #dt_fpr, dt_tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\n",
    "        #self.roc_auc = auc(dt_fpr, dt_tpr)\n",
    "        self.roc_auc = roc_auc_score(np.array(self.y_test_),np.array(self.y_pred_prob))\n",
    "\n",
    "        print(\"Model: {}\"  .format(self.model))\n",
    "        print(\"Best Param: {}\"  .format(self.best_param))\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"Accuracy:\\t%.4f\" % (self.accuracy))\n",
    "        print(\"Precision:\\t%.4f\" % (self.precision))\n",
    "        print(\"Recall:\\t\\t%.4f\" % (self.recall))\n",
    "        print(\"F1 Score:\\t%.4f\" % (2 * (self.precision * self.recall) / (self.precision + self.recall)))\n",
    "        print(\"roc_auc:\\t%.4f\" % (self.roc_auc))\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test_, self.y_pred).ravel()\n",
    "        print(\"True Negatives: %s\" % tn)\n",
    "        print(\"False Positives: %s\" % fp)\n",
    "        print(\"False Negatives: %s\" % fn)\n",
    "        print(\"True Positives: %s\" % tp)\n",
    "        self.tn = tn\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "        self.tp = tp\n",
    "        \n",
    "    def store_result (self):\n",
    "        result_dict = {\n",
    "            'Model': self.model,\n",
    "            \"Best_Param\": self.best_param,\n",
    "            'Accuracy': self.accuracy,\n",
    "            'Precision': self.precision,\n",
    "            'Recall': self.recall,\n",
    "            \"True Negatives\": self.tn,\n",
    "            \"False Positives\": self.fp,\n",
    "            \"False Negatives\": self.fn,\n",
    "            \"True Positives\": self.tp,\n",
    "            'F1 Score': self.f1,\n",
    "            \"ROC AUC\" : self.roc_auc}\n",
    "        results.append ([self.model, self.best_param, self.accuracy, self.precision, self.recall, self.tn, self.fp, self.fn, self.tp, self.f1, self.roc_auc])\n",
    "        results_df = pd.DataFrame(results,columns = ['Model', \"Best_Param\", 'Accuracy', 'Precision', 'Recall', \"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\", 'F1 Score', \"ROC AUC\"])\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb2c480a-090a-4cfb-b12e-3b74c325826c",
    "_uuid": "1b63bfd92008043cc1a336f924c835e73792f6d8"
   },
   "source": [
    "### Classifiers \n",
    "\n",
    "-As the dataset is extremely imbalanced, we evaluate our models using f1 and confusion matrix instead of roc_auc_score.<br>\n",
    "-Logistic Regression appears to perform better with oversampling dataset while adaboost and random forest performs better with undersampling dataset.  \n",
    "-Our best model = logistic regression with oversampling dataset (without stratified kfolds).  The model is able to detect 5 out of 6 fraud csaes, with relatively lower no. of false positive signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use GridSearchCV to find the best parameters using undersample dataset. \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring=\"f1_micro\")\n",
    "grid_log_reg.fit(X_train_NM2, y_train_NM2)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg_NM2 = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, scoring=\"f1_micro\")\n",
    "grid_knears.fit(X_train_NM2, y_train_NM2)\n",
    "\n",
    "# KNears best estimator\n",
    "knears_neighbors_NM2 = grid_knears.best_estimator_\n",
    "\n",
    "# Adaboost Classifier\n",
    "ada_params = {\n",
    " 'n_estimators': [5,10,15,50],\n",
    " 'base_estimator__max_depth' : [1,5,10],\n",
    " 'algorithm' : ['SAMME',\"SAMME.R\"]\n",
    " }\n",
    "\n",
    "grid_ada = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), ada_params, scoring=\"f1_micro\")\n",
    "grid_ada.fit(X_train_NM2, y_train_NM2)\n",
    "\n",
    "# ada best estimator\n",
    "ada_clf_NM2 = grid_ada.best_estimator_\n",
    "\n",
    "# random forest Classifier\n",
    "rf_params = {\n",
    " 'n_estimators': [20, 50, 100],\n",
    " 'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    " 'min_samples_leaf': [1,4,5],\n",
    " 'max_features': ['auto'],\n",
    " 'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    " 'bootstrap': [True]}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, scoring=\"f1_micro\")\n",
    "grid_rf.fit(X_train_NM2, y_train_NM2)\n",
    "\n",
    "# ada best estimator\n",
    "rf_clf_NM2 = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters using oversample dataset. \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring=\"f1_micro\")\n",
    "grid_log_reg.fit(X_train_SMTO, y_train_SMTO)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg_SMTO = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, scoring=\"f1_micro\")\n",
    "grid_knears.fit(X_train_SMTO, y_train_SMTO)\n",
    "\n",
    "# KNears best estimator\n",
    "knears_neighbors_SMTO = grid_knears.best_estimator_\n",
    "\n",
    "# Adaboost Classifier\n",
    "ada_params = {\n",
    " 'n_estimators': [5,10,15,50],\n",
    " 'base_estimator__max_depth' : [1,5,10],\n",
    " 'algorithm' : ['SAMME',\"SAMME.R\"]\n",
    " }\n",
    "\n",
    "grid_ada = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), ada_params, scoring=\"f1_micro\")\n",
    "grid_ada.fit(X_train_SMTO, y_train_SMTO)\n",
    "\n",
    "# ada best estimator\n",
    "ada_clf_SMTO = grid_ada.best_estimator_\n",
    "\n",
    "# random forest Classifier\n",
    "rf_params = {\n",
    " 'n_estimators': [20, 50, 100],\n",
    " 'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    " 'min_samples_leaf': [1,4,5],\n",
    " 'max_features': ['auto'],\n",
    " 'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    " 'bootstrap': [True]}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, scoring=\"f1_micro\")\n",
    "grid_rf.fit(X_train_SMTO, y_train_SMTO)\n",
    "\n",
    "# ada best estimator\n",
    "rf_clf_SMTO = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters dataset that undergoes KFold, then undersampling\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring=\"f1_micro\")\n",
    "grid_log_reg.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg_kf_NM2 = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, scoring=\"f1_micro\")\n",
    "grid_knears.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "\n",
    "# KNears best estimator\n",
    "knears_neighbors_kf_NM2 = grid_knears.best_estimator_\n",
    "\n",
    "# Adaboost Classifier\n",
    "ada_params = {\n",
    " 'n_estimators': [5,10,15,50],\n",
    " 'base_estimator__max_depth' : [1,5,10],\n",
    " 'algorithm' : ['SAMME',\"SAMME.R\"]\n",
    " }\n",
    "\n",
    "grid_ada = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), ada_params, scoring=\"f1_micro\")\n",
    "grid_ada.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "\n",
    "# ada best estimator\n",
    "ada_clf_kf_NM2 = grid_ada.best_estimator_\n",
    "\n",
    "\n",
    "# random forest Classifier\n",
    "rf_params = {\n",
    " 'n_estimators': [20, 50, 100],\n",
    " 'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    " 'min_samples_leaf': [1,4,5],\n",
    " 'max_features': ['auto'],\n",
    " 'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    " 'bootstrap': [True]}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, scoring=\"f1_micro\")\n",
    "grid_rf.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "\n",
    "# rf best estimator\n",
    "rf_clf_kf_NM2 = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters dataset that undergoes KFold, then oversampling\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring=\"f1_micro\")\n",
    "grid_log_reg.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg_kf_SMTO = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, scoring=\"f1_micro\")\n",
    "grid_knears.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "\n",
    "# KNears best estimator\n",
    "knears_neighbors_kf_SMTO = grid_knears.best_estimator_\n",
    "\n",
    "# Adaboost Classifier\n",
    "ada_params = {\n",
    " 'n_estimators': [5,10,15,50],\n",
    " 'base_estimator__max_depth' : [1,5,10],\n",
    " 'algorithm' : ['SAMME',\"SAMME.R\"]\n",
    " }\n",
    "\n",
    "grid_ada = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), ada_params, scoring=\"f1_micro\")\n",
    "grid_ada.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "\n",
    "# ada best estimator\n",
    "ada_clf_kf_SMTO = grid_ada.best_estimator_\n",
    "\n",
    "# random forest Classifier\n",
    "rf_params = {\n",
    " 'n_estimators': [20, 50, 100],\n",
    " 'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    " 'min_samples_leaf': [1,4,5],\n",
    " 'max_features': ['auto'],\n",
    " 'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    " 'bootstrap': [True]}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, scoring=\"f1_micro\")\n",
    "grid_rf.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "\n",
    "# rf best estimator\n",
    "rf_clf_kf_SMTO = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"log_reg_NM2={}\".format(log_reg_NM2))\n",
    "\n",
    "print (\"ada_clf_NM2={}\".format(ada_clf_NM2))\n",
    "\n",
    "print (\"rf_clf_NM2={}\".format(rf_clf_NM2))\n",
    "       \n",
    "##----\n",
    "print (\"log_reg_SMTO={}\".format(log_reg_SMTO))\n",
    "\n",
    "print (\"ada_clf_SMTO={}\".format(ada_clf_SMTO))\n",
    "    \n",
    "print (\"rf_clf_SMTO={}\".format(rf_clf_SMTO))\n",
    "       \n",
    "       \n",
    "##----\n",
    "print (\"log_reg_kf_NM2={}\".format(log_reg_kf_NM2)) \n",
    "    \n",
    "print (\"ada_clf_kf_NM2={}\".format(ada_clf_kf_NM2)) \n",
    "       \n",
    "print (\"rf_clf_kf_NM2={}\".format(rf_clf_kf_NM2)) \n",
    "       \n",
    "##----\n",
    "print (\"log_reg_kf_SMTO={}\".format(log_reg_kf_SMTO))\n",
    "\n",
    "print (\"ada_clf_kf_SMTO={}\".format(ada_clf_kf_SMTO))\n",
    "    \n",
    "print (\"rf_clf_kf_SMTO={}\".format(rf_clf_kf_SMTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Param - F1 Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight={1:0.85,0:0.15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_NM2=LogisticRegression(C=0.001, class_weight=class_weight, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "ada_clf_NM2=AdaBoostClassifier(algorithm='SAMME',\n",
    "          base_estimator=DecisionTreeClassifier(class_weight=class_weight, criterion='gini', max_depth=1,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "rf_clf_NM2=RandomForestClassifier(bootstrap=True, class_weight=class_weight, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "log_reg_SMTO=LogisticRegression(C=100, class_weight=class_weight, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "ada_clf_SMTO=AdaBoostClassifier(algorithm='SAMME',\n",
    "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "rf_clf_SMTO=RandomForestClassifier(bootstrap=True, class_weight=class_weight, criterion='gini',\n",
    "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=5,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "log_reg_kf_NM2=LogisticRegression(C=0.001, class_weight=class_weight, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "ada_clf_kf_NM2=AdaBoostClassifier(algorithm='SAMME',\n",
    "          base_estimator=DecisionTreeClassifier(class_weight=class_weight, criterion='gini', max_depth=1,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "          learning_rate=1.0, n_estimators=10, random_state=None)\n",
    "rf_clf_kf_NM2=RandomForestClassifier(bootstrap=True, class_weight=class_weight, criterion='gini',\n",
    "            max_depth=1, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=3,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "log_reg_kf_SMTO=LogisticRegression(C=1000, class_weight=class_weight, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "ada_clf_kf_SMTO=AdaBoostClassifier(algorithm='SAMME',\n",
    "          base_estimator=DecisionTreeClassifier(class_weight=class_weight, criterion='gini', max_depth=10,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "rf_clf_kf_SMTO=RandomForestClassifier(bootstrap=True, class_weight=class_weight, criterion='gini',\n",
    "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=4,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "7810d0b9-b4e5-4b7f-909b-c127365b167c",
    "_kg_hide-input": true,
    "_uuid": "8dd4ea07fd60973fccabc2d46af28a09b0de9178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lg_SMTO\n",
      "Best Param: LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.6376\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.8333\n",
      "F1 Score:\t0.0015\n",
      "roc_auc:\t0.7340\n",
      "True Negatives: 11548\n",
      "False Positives: 6565\n",
      "False Negatives: 1\n",
      "True Positives: 5\n",
      "Model: lg_SMTO_kf\n",
      "Best Param: LogisticRegression(C=1000, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.5757\n",
      "Precision:\t0.0006\n",
      "Recall:\t\t0.7500\n",
      "F1 Score:\t0.0012\n",
      "roc_auc:\t0.6220\n",
      "True Negatives: 6951\n",
      "False Positives: 5124\n",
      "False Negatives: 1\n",
      "True Positives: 3\n",
      "Model: lg_NM2\n",
      "Best Param: LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.9997\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.5000\n",
      "True Negatives: 18113\n",
      "False Positives: 0\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: lg_NM2_kf\n",
      "Best Param: LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.9997\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.5000\n",
      "True Negatives: 12075\n",
      "False Positives: 0\n",
      "False Negatives: 4\n",
      "True Positives: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_Param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>True Neg</th>\n",
       "      <th>False Pos</th>\n",
       "      <th>False Neg</th>\n",
       "      <th>True Pos</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lg_SMTO</td>\n",
       "      <td>LogisticRegression(C=100, class_weight={1: 0.8...</td>\n",
       "      <td>0.637618</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>11548</td>\n",
       "      <td>6565</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.733985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>lg_SMTO_kf</td>\n",
       "      <td>LogisticRegression(C=1000, class_weight={1: 0....</td>\n",
       "      <td>0.575710</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6951</td>\n",
       "      <td>5124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.622050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lg_NM2</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18113</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>lg_NM2_kf</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12075</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model                                         Best_Param  Accuracy  \\\n",
       "0     lg_SMTO  LogisticRegression(C=100, class_weight={1: 0.8...  0.637618   \n",
       "1  lg_SMTO_kf  LogisticRegression(C=1000, class_weight={1: 0....  0.575710   \n",
       "2      lg_NM2  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "3   lg_NM2_kf  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "\n",
       "   Precision    Recall  True Neg  False Pos  False Neg  True Pos  F1 Score  \\\n",
       "0   0.000761  0.833333     11548       6565          1         5  0.001521   \n",
       "1   0.000585  0.750000      6951       5124          1         3  0.001169   \n",
       "2   0.000000  0.000000     18113          0          6         0       NaN   \n",
       "3   0.000000  0.000000     12075          0          4         0       NaN   \n",
       "\n",
       "    ROC AUC  \n",
       "0  0.733985  \n",
       "1  0.622050  \n",
       "2  0.500000  \n",
       "3  0.500000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with oversampled dataset\n",
    "lg_SMTO = log_reg_SMTO.fit(X_train_SMTO,y_train_SMTO)\n",
    "lg_SMTO_pred_prob = lg_SMTO.predict_proba(X_test)[:,1]\n",
    "lg_SMTO_pred = lg_SMTO.predict(X_test)\n",
    "lg_SMTO_test_ = y_test\n",
    "lrg_SMTO = MODEL_EVA (\"lg_SMTO\",log_reg_SMTO,lg_SMTO_test_, lg_SMTO_pred,lg_SMTO_pred_prob)\n",
    "lrg_SMTO.confusion_matrix()\n",
    "lrg_SMTO.store_result()\n",
    "\n",
    "#with Kfolded oversampled dataset\n",
    "lg_SMTO_kf = log_reg_kf_SMTO.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "lg_SMTO_kf_pred_prob = lg_SMTO_kf.predict_proba(X_test_KF)[:,1]\n",
    "lg_SMTO_kf_pred = lg_SMTO_kf.predict(X_test_KF)\n",
    "lg_SMTO_kf_test_ = y_test_KF\n",
    "lrg_SMTO_kf = MODEL_EVA (\"lg_SMTO_kf\",log_reg_kf_SMTO,lg_SMTO_kf_test_, lg_SMTO_kf_pred,lg_SMTO_kf_pred_prob)\n",
    "lrg_SMTO_kf.confusion_matrix()\n",
    "lrg_SMTO_kf.store_result()\n",
    "\n",
    "#with undersampled dataset\n",
    "lg_NM2 = log_reg_NM2.fit(X_train_NM2,y_train_NM2)\n",
    "lg_NM2_pred_prob = lg_NM2.predict_proba(X_test)[:,1]\n",
    "lg_NM2_pred = lg_NM2.predict(X_test)\n",
    "lg_NM2_test_ = y_test\n",
    "lrg_NM2 = MODEL_EVA (\"lg_NM2\",log_reg_NM2,lg_NM2_test_, lg_NM2_pred,lg_NM2_pred_prob)\n",
    "lrg_NM2.confusion_matrix()\n",
    "lrg_NM2.store_result()\n",
    "\n",
    "#with Kfolded undersampled dataset\n",
    "lg_NM2_kf = log_reg_kf_NM2.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "lg_NM2_kf_pred_prob = lg_NM2_kf.predict_proba(X_test_KF2)[:,1]\n",
    "lg_NM2_kf_pred = lg_NM2_kf.predict(X_test_KF2)\n",
    "lg_NM2_kf_test_ = y_test_KF2\n",
    "lrg_NM2_kf = MODEL_EVA (\"lg_NM2_kf\",log_reg_kf_NM2,lg_NM2_kf_test_ , lg_NM2_kf_pred,lg_NM2_kf_pred_prob)\n",
    "lrg_NM2_kf.confusion_matrix()\n",
    "lrg_NM2_kf.store_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ada_SMTO\n",
      "Best Param: AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "------------------------------------\n",
      "Accuracy:\t0.9980\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.5219\n",
      "True Negatives: 18083\n",
      "False Positives: 30\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: ada_SMTO_kf\n",
      "Best Param: AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight={1: 0.85, 0: 0.15}, criterion='gini',\n",
      "            max_depth=10, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "------------------------------------\n",
      "Accuracy:\t0.9984\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.8704\n",
      "True Negatives: 12060\n",
      "False Positives: 15\n",
      "False Negatives: 4\n",
      "True Positives: 0\n",
      "Model: ada_NM2\n",
      "Best Param: AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight={1: 0.85, 0: 0.15}, criterion='gini',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5895\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: ada_NM2_kf\n",
      "Best Param: AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight={1: 0.85, 0: 0.15}, criterion='gini',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None)\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5121\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_Param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>True Neg</th>\n",
       "      <th>False Pos</th>\n",
       "      <th>False Neg</th>\n",
       "      <th>True Pos</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lg_SMTO</td>\n",
       "      <td>LogisticRegression(C=100, class_weight={1: 0.8...</td>\n",
       "      <td>0.637618</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>11548</td>\n",
       "      <td>6565</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.733985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>lg_SMTO_kf</td>\n",
       "      <td>LogisticRegression(C=1000, class_weight={1: 0....</td>\n",
       "      <td>0.575710</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6951</td>\n",
       "      <td>5124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.622050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lg_NM2</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18113</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>lg_NM2_kf</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12075</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ada_SMTO</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18083</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>ada_SMTO_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.998427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12060</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>ada_NM2</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.426790</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7729</td>\n",
       "      <td>10384</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.589466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ada_NM2_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.432321</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5220</td>\n",
       "      <td>6855</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.512112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model                                         Best_Param  Accuracy  \\\n",
       "0      lg_SMTO  LogisticRegression(C=100, class_weight={1: 0.8...  0.637618   \n",
       "1   lg_SMTO_kf  LogisticRegression(C=1000, class_weight={1: 0....  0.575710   \n",
       "2       lg_NM2  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "3    lg_NM2_kf  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "4     ada_SMTO  (DecisionTreeClassifier(class_weight=None, cri...  0.998013   \n",
       "5  ada_SMTO_kf  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.998427   \n",
       "6      ada_NM2  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.426790   \n",
       "7   ada_NM2_kf  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.432321   \n",
       "\n",
       "   Precision    Recall  True Neg  False Pos  False Neg  True Pos  F1 Score  \\\n",
       "0   0.000761  0.833333     11548       6565          1         5  0.001521   \n",
       "1   0.000585  0.750000      6951       5124          1         3  0.001169   \n",
       "2   0.000000  0.000000     18113          0          6         0       NaN   \n",
       "3   0.000000  0.000000     12075          0          4         0       NaN   \n",
       "4   0.000000  0.000000     18083         30          6         0       NaN   \n",
       "5   0.000000  0.000000     12060         15          4         0       NaN   \n",
       "6   0.000385  0.666667      7729      10384          2         4  0.000770   \n",
       "7   0.000292  0.500000      5220       6855          2         2  0.000583   \n",
       "\n",
       "    ROC AUC  \n",
       "0  0.733985  \n",
       "1  0.622050  \n",
       "2  0.500000  \n",
       "3  0.500000  \n",
       "4  0.521886  \n",
       "5  0.870352  \n",
       "6  0.589466  \n",
       "7  0.512112  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with oversampled dataset\n",
    "ada_SMTO = ada_clf_SMTO.fit(X_train_SMTO,y_train_SMTO)\n",
    "ada_SMTO_pred_prob =ada_SMTO.predict_proba(X_test)[:,1]\n",
    "ada_SMTO_pred = ada_SMTO.predict(X_test)\n",
    "ada_SMTO_test_ = y_test\n",
    "adag_SMTO = MODEL_EVA (\"ada_SMTO\",ada_clf_SMTO,ada_SMTO_test_, ada_SMTO_pred,ada_SMTO_pred_prob)\n",
    "adag_SMTO.confusion_matrix()\n",
    "adag_SMTO.store_result()\n",
    "\n",
    "#with Kfolded oversampled dataset\n",
    "ada_SMTO_kf = ada_clf_kf_SMTO.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "ada_SMTO_kf_pred_prob = ada_SMTO_kf.predict_proba(X_test_KF)[:,1]\n",
    "ada_SMTO_kf_pred = ada_SMTO_kf.predict(X_test_KF)\n",
    "ada_SMTO_kf_test_ = y_test_KF\n",
    "adag_SMTO_kf = MODEL_EVA (\"ada_SMTO_kf\",ada_clf_kf_SMTO,ada_SMTO_kf_test_, ada_SMTO_kf_pred,ada_SMTO_kf_pred_prob)\n",
    "adag_SMTO_kf.confusion_matrix()\n",
    "adag_SMTO_kf.store_result()\n",
    "\n",
    "#with undersampled dataset\n",
    "ada_NM2 = ada_clf_NM2.fit(X_train_NM2,y_train_NM2)\n",
    "ada_NM2_pred_prob = ada_NM2.predict_proba(X_test)[:,1]\n",
    "ada_NM2_pred = ada_NM2.predict(X_test)\n",
    "ada_NM2_test_ = y_test\n",
    "adag_NM2 = MODEL_EVA (\"ada_NM2\",ada_clf_NM2,ada_NM2_test_, ada_NM2_pred,ada_NM2_pred_prob)\n",
    "adag_NM2.confusion_matrix()\n",
    "adag_NM2.store_result()\n",
    "\n",
    "#with Kfolded undersampled dataset\n",
    "ada_NM2_kf = ada_clf_kf_NM2.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "ada_NM2_kf_pred_prob = ada_NM2_kf.predict_proba(X_test_KF2)[:,1]\n",
    "ada_NM2_kf_pred = ada_NM2_kf.predict(X_test_KF2)\n",
    "ada_NM2_kf_test_ = y_test_KF2\n",
    "adag_NM2_kf = MODEL_EVA (\"ada_NM2_kf\",ada_clf_kf_NM2,ada_NM2_kf_test_, ada_NM2_kf_pred,ada_NM2_kf_pred_prob)\n",
    "adag_NM2_kf.confusion_matrix()\n",
    "adag_NM2_kf.store_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: rf_SMTO\n",
      "Best Param: RandomForestClassifier(bootstrap=True, class_weight={1: 0.85, 0: 0.15},\n",
      "            criterion='gini', max_depth=10, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.8608\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0016\n",
      "roc_auc:\t0.4680\n",
      "True Negatives: 15595\n",
      "False Positives: 2518\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: rf_SMTO_kf\n",
      "Best Param: RandomForestClassifier(bootstrap=True, class_weight={1: 0.85, 0: 0.15},\n",
      "            criterion='gini', max_depth=10, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.9367\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6917\n",
      "True Negatives: 11314\n",
      "False Positives: 761\n",
      "False Negatives: 4\n",
      "True Positives: 0\n",
      "Model: rf_NM2\n",
      "Best Param: RandomForestClassifier(bootstrap=True, class_weight={1: 0.85, 0: 0.15},\n",
      "            criterion='gini', max_depth=2, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.4403\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5961\n",
      "True Negatives: 7973\n",
      "False Positives: 10140\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: rf_NM2_kf\n",
      "Best Param: RandomForestClassifier(bootstrap=True, class_weight={1: 0.85, 0: 0.15},\n",
      "            criterion='gini', max_depth=1, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "------------------------------------\n",
      "Accuracy:\t0.5909\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5385\n",
      "True Negatives: 7136\n",
      "False Positives: 4939\n",
      "False Negatives: 2\n",
      "True Positives: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_Param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>True Neg</th>\n",
       "      <th>False Pos</th>\n",
       "      <th>False Neg</th>\n",
       "      <th>True Pos</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lg_SMTO</td>\n",
       "      <td>LogisticRegression(C=100, class_weight={1: 0.8...</td>\n",
       "      <td>0.637618</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>11548</td>\n",
       "      <td>6565</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.733985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>lg_SMTO_kf</td>\n",
       "      <td>LogisticRegression(C=1000, class_weight={1: 0....</td>\n",
       "      <td>0.575710</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6951</td>\n",
       "      <td>5124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.622050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lg_NM2</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18113</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>lg_NM2_kf</td>\n",
       "      <td>LogisticRegression(C=0.001, class_weight={1: 0...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12075</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ada_SMTO</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18083</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>ada_SMTO_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.998427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12060</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>ada_NM2</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.426790</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7729</td>\n",
       "      <td>10384</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.589466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ada_NM2_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight={1: 0.85,...</td>\n",
       "      <td>0.432321</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5220</td>\n",
       "      <td>6855</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.512112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>rf_SMTO</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.860809</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15595</td>\n",
       "      <td>2518</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.468025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>rf_SMTO_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11314</td>\n",
       "      <td>761</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.691718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>rf_NM2</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.440256</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7973</td>\n",
       "      <td>10140</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.596054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>rf_NM2_kf</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.590943</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7136</td>\n",
       "      <td>4939</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.538530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                         Best_Param  Accuracy  \\\n",
       "0       lg_SMTO  LogisticRegression(C=100, class_weight={1: 0.8...  0.637618   \n",
       "1    lg_SMTO_kf  LogisticRegression(C=1000, class_weight={1: 0....  0.575710   \n",
       "2        lg_NM2  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "3     lg_NM2_kf  LogisticRegression(C=0.001, class_weight={1: 0...  0.999669   \n",
       "4      ada_SMTO  (DecisionTreeClassifier(class_weight=None, cri...  0.998013   \n",
       "5   ada_SMTO_kf  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.998427   \n",
       "6       ada_NM2  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.426790   \n",
       "7    ada_NM2_kf  (DecisionTreeClassifier(class_weight={1: 0.85,...  0.432321   \n",
       "8       rf_SMTO  (DecisionTreeClassifier(class_weight=None, cri...  0.860809   \n",
       "9    rf_SMTO_kf  (DecisionTreeClassifier(class_weight=None, cri...  0.936667   \n",
       "10       rf_NM2  (DecisionTreeClassifier(class_weight=None, cri...  0.440256   \n",
       "11    rf_NM2_kf  (DecisionTreeClassifier(class_weight=None, cri...  0.590943   \n",
       "\n",
       "    Precision    Recall  True Neg  False Pos  False Neg  True Pos  F1 Score  \\\n",
       "0    0.000761  0.833333     11548       6565          1         5  0.001521   \n",
       "1    0.000585  0.750000      6951       5124          1         3  0.001169   \n",
       "2    0.000000  0.000000     18113          0          6         0       NaN   \n",
       "3    0.000000  0.000000     12075          0          4         0       NaN   \n",
       "4    0.000000  0.000000     18083         30          6         0       NaN   \n",
       "5    0.000000  0.000000     12060         15          4         0       NaN   \n",
       "6    0.000385  0.666667      7729      10384          2         4  0.000770   \n",
       "7    0.000292  0.500000      5220       6855          2         2  0.000583   \n",
       "8    0.000794  0.333333     15595       2518          4         2  0.001584   \n",
       "9    0.000000  0.000000     11314        761          4         0       NaN   \n",
       "10   0.000394  0.666667      7973      10140          2         4  0.000788   \n",
       "11   0.000405  0.500000      7136       4939          2         2  0.000809   \n",
       "\n",
       "     ROC AUC  \n",
       "0   0.733985  \n",
       "1   0.622050  \n",
       "2   0.500000  \n",
       "3   0.500000  \n",
       "4   0.521886  \n",
       "5   0.870352  \n",
       "6   0.589466  \n",
       "7   0.512112  \n",
       "8   0.468025  \n",
       "9   0.691718  \n",
       "10  0.596054  \n",
       "11  0.538530  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with oversampled dataset\n",
    "rf_SMTO = rf_clf_SMTO.fit(X_train_SMTO,y_train_SMTO)\n",
    "rf_SMTO_pred_prob =rf_SMTO.predict_proba(X_test)[:,1]\n",
    "rf_SMTO_pred = rf_SMTO.predict(X_test)\n",
    "rf_SMTO_test_ = y_test\n",
    "rfg_SMTO = MODEL_EVA (\"rf_SMTO\",rf_clf_SMTO,rf_SMTO_test_, rf_SMTO_pred,rf_SMTO_pred_prob)\n",
    "rfg_SMTO.confusion_matrix()\n",
    "rfg_SMTO.store_result()\n",
    "\n",
    "#with Kfolded oversampled dataset\n",
    "rf_SMTO_kf = rf_clf_kf_SMTO.fit(X_train_KF_SMTO,y_train_KF_SMTO)\n",
    "rf_SMTO_kf_pred_prob = rf_SMTO_kf.predict_proba(X_test_KF)[:,1]\n",
    "rf_SMTO_kf_pred = rf_SMTO_kf.predict(X_test_KF)\n",
    "rf_SMTO_kf_test_ = y_test_KF\n",
    "rfg_SMTO_kf = MODEL_EVA (\"rf_SMTO_kf\",rf_clf_kf_SMTO,rf_SMTO_kf_test_, rf_SMTO_kf_pred,rf_SMTO_kf_pred_prob)\n",
    "rfg_SMTO_kf.confusion_matrix()\n",
    "rfg_SMTO_kf.store_result()\n",
    "\n",
    "#with undersampled dataset\n",
    "rf_NM2 = rf_clf_NM2.fit(X_train_NM2,y_train_NM2)\n",
    "rf_NM2_pred_prob = rf_NM2.predict_proba(X_test)[:,1]\n",
    "rf_NM2_pred = rf_NM2.predict(X_test)\n",
    "rf_NM2_test_ = y_test\n",
    "rfg_NM2 = MODEL_EVA (\"rf_NM2\",rf_clf_NM2,rf_NM2_test_, rf_NM2_pred,rf_NM2_pred_prob)\n",
    "rfg_NM2.confusion_matrix()\n",
    "rfg_NM2.store_result()\n",
    "\n",
    "#with Kfolded undersampled dataset\n",
    "rf_NM2_kf = rf_clf_kf_NM2.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "rf_NM2_kf_pred_prob = rf_NM2_kf.predict_proba(X_test_KF2)[:,1]\n",
    "rf_NM2_kf_pred = rf_NM2_kf.predict(X_test_KF2)\n",
    "rf_NM2_kf_test_ = y_test_KF2\n",
    "rfg_NM2_kf = MODEL_EVA (\"rf_NM2_kf\",rf_clf_kf_NM2,rf_NM2_kf_test_, rf_NM2_kf_pred,rf_NM2_kf_pred_prob)\n",
    "rfg_NM2_kf.confusion_matrix()\n",
    "rfg_NM2_kf.store_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best weights for ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_NM2_kf_112\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5555\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_113\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5688\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_121\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5265\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0007\n",
      "roc_auc:\t0.5545\n",
      "True Negatives: 6357\n",
      "False Positives: 5718\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_122\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4358\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5339\n",
      "True Negatives: 5262\n",
      "False Positives: 6813\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_123\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5319\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_131\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5569\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0007\n",
      "roc_auc:\t0.5750\n",
      "True Negatives: 6725\n",
      "False Positives: 5350\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_132\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4357\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5784\n",
      "True Negatives: 5261\n",
      "False Positives: 6814\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_133\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4501\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5678\n",
      "True Negatives: 5435\n",
      "False Positives: 6640\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_211\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5006\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0007\n",
      "roc_auc:\t0.5535\n",
      "True Negatives: 6045\n",
      "False Positives: 6030\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_212\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5413\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_213\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5671\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_NM2_kf_221\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5036\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0007\n",
      "roc_auc:\t0.5452\n",
      "True Negatives: 6081\n",
      "False Positives: 5994\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_223\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5577\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_231\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5642\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5097\n",
      "True Negatives: 6813\n",
      "False Positives: 5262\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_232\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5509\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_233\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4627\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5693\n",
      "True Negatives: 5587\n",
      "False Positives: 6488\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_311\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4360\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5400\n",
      "True Negatives: 5265\n",
      "False Positives: 6810\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_312\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5686\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_313\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5606\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_321\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5717\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5712\n",
      "True Negatives: 6903\n",
      "False Positives: 5172\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_322\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4323\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5508\n",
      "True Negatives: 5220\n",
      "False Positives: 6855\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_323\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4353\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0006\n",
      "roc_auc:\t0.5530\n",
      "True Negatives: 5256\n",
      "False Positives: 6819\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_331\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5906\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:\t0.5564\n",
      "True Negatives: 7132\n",
      "False Positives: 4943\n",
      "False Negatives: 2\n",
      "True Positives: 2\n",
      "Model: VC_NM2_kf_332\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2_kf', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=F...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4987\n",
      "Precision:\t0.0003\n",
      "Recall:\t\t0.5000\n",
      "F1 Score:\t0.0007\n",
      "roc_auc:\t0.5636\n",
      "True Negatives: 6022\n",
      "False Positives: 6053\n",
      "False Negatives: 2\n",
      "True Positives: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "vc_weights_NM2KF = pd.DataFrame(columns=('w1', 'w2', 'w3', 'FP', 'TP'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "\n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "\n",
    "            eclf = VotingClassifier(estimators=[ (\"lg_NM2_kf\", lg_NM2_kf),(\"rf_NM2_kf\", rf_NM2_kf), (\"ada_NM2_kf\", ada_NM2_kf)], weights=[w1,w2,w3], voting=\"soft\")\n",
    "            eclf= eclf.fit(X_train_KF_NM2,y_train_KF_NM2)\n",
    "            eclf_pred_prob = eclf.predict_proba(X_test_KF2)[:,1]\n",
    "            eclf_pred = eclf.predict(X_test_KF2)\n",
    "            eclf_test = y_test_KF2\n",
    "            VC_eva = MODEL_EVA (\"VC_NM2_kf_{}{}{}\".format(w1,w2,w3),eclf,eclf_test, eclf_pred,eclf_pred_prob)\n",
    "            VC_eva.confusion_matrix()\n",
    "            VC_eva.store_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_NM2_112\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4289\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6535\n",
      "True Negatives: 7768\n",
      "False Positives: 10345\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_113\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5968\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_121\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4646\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5926\n",
      "True Negatives: 8414\n",
      "False Positives: 9699\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_122\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4296\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5749\n",
      "True Negatives: 7780\n",
      "False Positives: 10333\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_123\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6539\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_131\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4595\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6185\n",
      "True Negatives: 8322\n",
      "False Positives: 9791\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_132\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4662\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6416\n",
      "True Negatives: 8443\n",
      "False Positives: 9670\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_133\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4427\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6404\n",
      "True Negatives: 8018\n",
      "False Positives: 10095\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_211\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4289\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6060\n",
      "True Negatives: 7768\n",
      "False Positives: 10345\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_212\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6193\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_213\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5870\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_221\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4294\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5129\n",
      "True Negatives: 7776\n",
      "False Positives: 10337\n",
      "False Negatives: 2\n",
      "True Positives: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_NM2_223\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5644\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_231\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4696\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5876\n",
      "True Negatives: 8505\n",
      "False Positives: 9608\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_232\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4491\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6541\n",
      "True Negatives: 8133\n",
      "False Positives: 9980\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_233\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4292\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6077\n",
      "True Negatives: 7772\n",
      "False Positives: 10341\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_311\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4304\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5978\n",
      "True Negatives: 7795\n",
      "False Positives: 10318\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_312\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6721\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_313\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6316\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_321\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4133\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.5599\n",
      "True Negatives: 7484\n",
      "False Positives: 10629\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_322\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4281\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6594\n",
      "True Negatives: 7753\n",
      "False Positives: 10360\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_323\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.4268\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0008\n",
      "roc_auc:\t0.6105\n",
      "True Negatives: 7729\n",
      "False Positives: 10384\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_331\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5016\n",
      "Precision:\t0.0004\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0009\n",
      "roc_auc:\t0.5835\n",
      "True Negatives: 9084\n",
      "False Positives: 9029\n",
      "False Negatives: 2\n",
      "True Positives: 4\n",
      "Model: VC_NM2_332\n",
      "Best Param: VotingClassifier(estimators=[('lg_NM2', LogisticRegression(C=0.001, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=Fals...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.5288\n",
      "Precision:\t0.0005\n",
      "Recall:\t\t0.6667\n",
      "F1 Score:\t0.0009\n",
      "roc_auc:\t0.6156\n",
      "True Negatives: 9578\n",
      "False Positives: 8535\n",
      "False Negatives: 2\n",
      "True Positives: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "vc_weights_NM2 = pd.DataFrame(columns=('w1', 'w2', 'w3', 'FP', 'TP'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "\n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "\n",
    "            eclf_ = VotingClassifier(estimators=[ (\"lg_NM2\", lg_NM2),(\"rf_NM2\", rf_NM2), (\"ada_NM2\", ada_NM2)], weights=[w1,w2,w3], voting=\"soft\")\n",
    "            eclf_ = eclf_.fit(X_train_NM2,y_train_NM2)\n",
    "            eclf_pred_prob = eclf_.predict_proba(X_test)[:,1]\n",
    "            eclf_pred = eclf_.predict(X_test)\n",
    "            eclf_test_ = y_test\n",
    "            VC_eva = MODEL_EVA (\"VC_NM2_{}{}{}\".format(w1,w2,w3),eclf_,eclf_test_ , eclf_pred,eclf_pred_prob)\n",
    "            VC_eva.confusion_matrix()\n",
    "            VC_eva.store_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_SMTO_112\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9555\n",
      "Precision:\t0.0012\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0025\n",
      "roc_auc:\t0.6789\n",
      "True Negatives: 17312\n",
      "False Positives: 801\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_113\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9800\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6579\n",
      "True Negatives: 17756\n",
      "False Positives: 357\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_121\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8767\n",
      "Precision:\t0.0009\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0018\n",
      "roc_auc:\t0.6453\n",
      "True Negatives: 15883\n",
      "False Positives: 2230\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_122\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9273\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0015\n",
      "roc_auc:\t0.6321\n",
      "True Negatives: 16800\n",
      "False Positives: 1313\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_123\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9526\n",
      "Precision:\t0.0012\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0023\n",
      "roc_auc:\t0.6407\n",
      "True Negatives: 17260\n",
      "False Positives: 853\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_131\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8655\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0016\n",
      "roc_auc:\t0.6414\n",
      "True Negatives: 15680\n",
      "False Positives: 2433\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_132\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8951\n",
      "Precision:\t0.0011\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0021\n",
      "roc_auc:\t0.6410\n",
      "True Negatives: 16216\n",
      "False Positives: 1897\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_133\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[1, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9276\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0015\n",
      "roc_auc:\t0.6356\n",
      "True Negatives: 16806\n",
      "False Positives: 1307\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_211\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8908\n",
      "Precision:\t0.0010\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0020\n",
      "roc_auc:\t0.6977\n",
      "True Negatives: 16138\n",
      "False Positives: 1975\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_212\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9275\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6658\n",
      "True Negatives: 16806\n",
      "False Positives: 1307\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_213\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9521\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6804\n",
      "True Negatives: 17252\n",
      "False Positives: 861\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_221\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8914\n",
      "Precision:\t0.0010\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0020\n",
      "roc_auc:\t0.6757\n",
      "True Negatives: 16149\n",
      "False Positives: 1964\n",
      "False Negatives: 4\n",
      "True Positives: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VC_SMTO_223\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9374\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6381\n",
      "True Negatives: 16985\n",
      "False Positives: 1128\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_231\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8498\n",
      "Precision:\t0.0007\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0015\n",
      "roc_auc:\t0.6403\n",
      "True Negatives: 15396\n",
      "False Positives: 2717\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_232\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8927\n",
      "Precision:\t0.0005\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0010\n",
      "roc_auc:\t0.6418\n",
      "True Negatives: 16174\n",
      "False Positives: 1939\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_233\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[2, 3, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9260\n",
      "Precision:\t0.0007\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0015\n",
      "roc_auc:\t0.6419\n",
      "True Negatives: 16778\n",
      "False Positives: 1335\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_311\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8829\n",
      "Precision:\t0.0005\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0009\n",
      "roc_auc:\t0.7013\n",
      "True Negatives: 15997\n",
      "False Positives: 2116\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_312\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9113\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.6992\n",
      "True Negatives: 16512\n",
      "False Positives: 1601\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_313\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 1, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9318\n",
      "Precision:\t0.0000\n",
      "Recall:\t\t0.0000\n",
      "F1 Score:\tnan\n",
      "roc_auc:\t0.7107\n",
      "True Negatives: 16884\n",
      "False Positives: 1229\n",
      "False Negatives: 6\n",
      "True Positives: 0\n",
      "Model: VC_SMTO_321\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8694\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0017\n",
      "roc_auc:\t0.6625\n",
      "True Negatives: 15750\n",
      "False Positives: 2363\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_322\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8965\n",
      "Precision:\t0.0005\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0011\n",
      "roc_auc:\t0.6519\n",
      "True Negatives: 16243\n",
      "False Positives: 1870\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_323\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 2, 3])\n",
      "------------------------------------\n",
      "Accuracy:\t0.9197\n",
      "Precision:\t0.0007\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0014\n",
      "roc_auc:\t0.7100\n",
      "True Negatives: 16663\n",
      "False Positives: 1450\n",
      "False Negatives: 5\n",
      "True Positives: 1\n",
      "Model: VC_SMTO_331\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 1])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8601\n",
      "Precision:\t0.0008\n",
      "Recall:\t\t0.3333\n",
      "F1 Score:\t0.0016\n",
      "roc_auc:\t0.6697\n",
      "True Negatives: 15582\n",
      "False Positives: 2531\n",
      "False Negatives: 4\n",
      "True Positives: 2\n",
      "Model: VC_SMTO_332\n",
      "Best Param: VotingClassifier(estimators=[('lg_SMTO', LogisticRegression(C=100, class_weight={1: 0.85, 0: 0.15}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False...ne,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft',\n",
      "         weights=[3, 3, 2])\n",
      "------------------------------------\n",
      "Accuracy:\t0.8913\n",
      "Precision:\t0.0005\n",
      "Recall:\t\t0.1667\n",
      "F1 Score:\t0.0010\n",
      "roc_auc:\t0.6556\n",
      "True Negatives: 16148\n",
      "False Positives: 1965\n",
      "False Negatives: 5\n",
      "True Positives: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "vc_weights_SMTO = pd.DataFrame(columns=('w1', 'w2', 'w3', 'FP', 'TP'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "\n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "\n",
    "            eclf_ = VotingClassifier(estimators=[ (\"lg_SMTO\", lg_SMTO),(\"rf_SMTO\", rf_SMTO), (\"ada_SMTO\", ada_SMTO)], weights=[w1,w2,w3], voting=\"soft\")\n",
    "            eclf_ = eclf_.fit(X_train_SMTO,y_train_SMTO)\n",
    "            eclf_pred_prob = eclf_.predict_proba(X_test)[:,1]\n",
    "            eclf_pred = eclf_.predict(X_test)\n",
    "            eclf_test_ = y_test\n",
    "            VC_eva = MODEL_EVA (\"VC_SMTO_{}{}{}\".format(w1,w2,w3),eclf_,eclf_test_ , eclf_pred,eclf_pred_prob)\n",
    "            VC_eva.confusion_matrix()\n",
    "            VC_eva.store_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results,columns = ['Model', \"Best_Param\", 'Accuracy', 'Precision', 'Recall', \"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\", 'F1 Score', \"ROC AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
